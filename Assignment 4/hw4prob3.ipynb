{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 Homework 4 CSE 250A Probabilistic Reasoning & Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** \n",
    " \n",
    "Compute the maximum likelihood estimate of the unigram distribution $P_{u}(w)$ over words $w$. Print out\n",
    "a table of all the tokens (i.e., words) that start with the letter “M”, along with their numerical unigram\n",
    "probabilities (not counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Unigram Probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>MILLION</td>\n",
       "      <td>0.002073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>MORE</td>\n",
       "      <td>0.001709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>MR.</td>\n",
       "      <td>0.001442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>MOST</td>\n",
       "      <td>0.000788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>MARKET</td>\n",
       "      <td>0.000780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>MAY</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>M.</td>\n",
       "      <td>0.000703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>MANY</td>\n",
       "      <td>0.000697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>MADE</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>MUCH</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>MAKE</td>\n",
       "      <td>0.000514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>MONTH</td>\n",
       "      <td>0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>MONEY</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>MONTHS</td>\n",
       "      <td>0.000406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>MY</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>MONDAY</td>\n",
       "      <td>0.000382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>MAJOR</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>MILITARY</td>\n",
       "      <td>0.000352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>MEMBERS</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>MIGHT</td>\n",
       "      <td>0.000274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>MEETING</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>MUST</td>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>ME</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>MARCH</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>MAN</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>MS.</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>MINISTER</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>MAKING</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>MOVE</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>MILES</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Words  Unigram Probs\n",
       "53    MILLION       0.002073\n",
       "68       MORE       0.001709\n",
       "76        MR.       0.001442\n",
       "120      MOST       0.000788\n",
       "121    MARKET       0.000780\n",
       "125       MAY       0.000730\n",
       "129        M.       0.000703\n",
       "130      MANY       0.000697\n",
       "158      MADE       0.000560\n",
       "177      MUCH       0.000515\n",
       "179      MAKE       0.000514\n",
       "202     MONTH       0.000445\n",
       "208     MONEY       0.000437\n",
       "226    MONTHS       0.000406\n",
       "229        MY       0.000400\n",
       "246    MONDAY       0.000382\n",
       "255     MAJOR       0.000371\n",
       "274  MILITARY       0.000352\n",
       "286   MEMBERS       0.000336\n",
       "355     MIGHT       0.000274\n",
       "365   MEETING       0.000266\n",
       "369      MUST       0.000267\n",
       "373        ME       0.000264\n",
       "374     MARCH       0.000260\n",
       "384       MAN       0.000253\n",
       "402       MS.       0.000239\n",
       "403  MINISTER       0.000240\n",
       "459    MAKING       0.000212\n",
       "472      MOVE       0.000210\n",
       "478     MILES       0.000206"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Import vocabulary and unigrams as pandas dataframes\n",
    "vocab_data = pd.read_csv('hw4_vocab.txt',sep=\"\\t\",quoting=csv.QUOTE_NONE,header=None,names=[\"Words\"])\n",
    "unigram_data = pd.read_csv('hw4_unigram.txt',sep=\"\\t\", header=None,names=[\"Count\"])\n",
    "\n",
    "# Add a column for the unigram probabilities in the vocab dataframe\n",
    "vocab_data[\"Unigram Probs\"] = unigram_data[\"Count\"]/unigram_data[\"Count\"].sum()\n",
    "\n",
    "# Print the words starting with M and their probabilities\n",
    "vocab_data[vocab_data[\"Words\"].str.startswith(\"M\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\n",
    "\n",
    "Compute the maximum likelihood estimate of the bigram distribution $P_{b}(w'|w)$. Print out a table of\n",
    "the ten most likely words to follow the word “THE”, along with their numerical bigram probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      w1   w2  count(w1,w2)  Bigram Probs\n",
      "993    4    1       2371132      0.615020\n",
      "1058   4   70         51556      0.013372\n",
      "1064   4   79         45186      0.011720\n",
      "1060   4   73         44949      0.011659\n",
      "1050   4   61         36439      0.009451\n",
      "1165   4  184         33435      0.008672\n",
      "1086   4  103         26230      0.006803\n",
      "1029   4   39         25641      0.006651\n",
      "1282   4  308         24239      0.006287\n",
      "1014   4   23         23752      0.006161\n",
      "\n",
      " We access the words from the vocab dataframe to print them out nicely\n",
      "Word: <UNK> \t \t Probability: 0.61502\n",
      "Word: U. \t \t Probability: 0.013372\n",
      "Word: FIRST \t \t Probability: 0.01172\n",
      "Word: COMPANY \t \t Probability: 0.011659\n",
      "Word: NEW \t \t Probability: 0.009451\n",
      "Word: UNITED \t \t Probability: 0.008672\n",
      "Word: GOVERNMENT \t Probability: 0.006803\n",
      "Word: NINETEEN \t \t Probability: 0.006651\n",
      "Word: SAME \t \t Probability: 0.006287\n",
      "Word: TWO \t \t Probability: 0.006161\n"
     ]
    }
   ],
   "source": [
    "# Read the bigram file as a dataframe\n",
    "bigram_data = pd.read_csv('hw4_bigram.txt',sep=\"\\t\",header=None,names=[\"w1\",\"w2\",\"count(w1,w2)\"])\n",
    "# Retrieve the index of the word \"THE\" in the previous dataframe\n",
    "index = vocab_data.index\n",
    "condition = vocab_data[\"Words\"] == \"THE\"\n",
    "the_index_list = index[condition]\n",
    "the_index = the_index_list[0]+1\n",
    "\n",
    "# Make a new dataframe with only the \"THE\" word as \"w1\" \n",
    "the_bigram = bigram_data[bigram_data[\"w1\"]==the_index]\n",
    "\n",
    "# Save the bigram probabilities as a column in the bigram dataframe\n",
    "word_sum = the_bigram[\"count(w1,w2)\"].sum()\n",
    "the_bigram[\"Bigram Probs\"] = the_bigram[\"count(w1,w2)\"]/word_sum\n",
    "the_bigram = the_bigram.sort_values(\"Bigram Probs\", ascending=False)\n",
    "ten_words_after_the = the_bigram.head(10)\n",
    "print(ten_words_after_the)\n",
    "\n",
    "print(\"\\n We access the words from the vocab dataframe to print them out nicely\")\n",
    "\n",
    "# Save words and probabilities as lists to print out nicely\n",
    "bigram_prob_list = ten_words_after_the[\"Bigram Probs\"].tolist()\n",
    "second_word = []\n",
    "# Add all second words to list to easily print out\n",
    "second_word.extend((vocab_data.loc[0,\"Words\"],vocab_data.loc[69,\"Words\"],vocab_data.loc[78,\"Words\"],vocab_data.loc[72,\"Words\"],vocab_data.loc[60,\"Words\"],vocab_data.loc[183,\"Words\"],vocab_data.loc[102,\"Words\"],vocab_data.loc[38,\"Words\"],vocab_data.loc[307,\"Words\"],vocab_data.loc[22,\"Words\"]))\n",
    "for i in range(len(second_word)):\n",
    "    if (second_word[i]==\"GOVERNMENT\"):\n",
    "        print(\"Word: {} \\t Probability: {}\".format(second_word[i], round(bigram_prob_list[i],6)))\n",
    "    else:\n",
    "        print(\"Word: {} \\t \\t Probability: {}\".format(second_word[i], round(bigram_prob_list[i],6)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**\n",
    "\n",
    "Consider the sentence **“The stock market fell by one hundred points last week.”** Ignoring punctuation,\n",
    "compute and compare the log-likelihoods of this sentence under the unigram and bigram models:\n",
    "\n",
    "$$\\mathcal{L}_{u} = log[P_{u}(\\bf(The))P_{u}(\\bf(stock))P_{u}(\\bf(market))...P_{u}(\\bf(last))P_{u}(\\bf(week))]$$\n",
    "$$\\mathcal{L}_{b} = log[P_{b}(\\bf(The|<s>))P_{b}(\\bf(stock|the))P_{b}(\\bf(market|stock))...P_{b}(\\bf(last|points))P_{b}(\\bf(week|last))]$$\n",
    "\n",
    "In the equation for the bigram log-likelihood, the token $<s>$ is used to mark the beginning of a sentence.\n",
    "Which model yields the highest log-likelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I see now that I should have saved bigram probabilities for all words, so I will do that\n",
    "bigram_probs = []\n",
    "for index,row in bigram_data.iterrows():\n",
    "    bigram_probs.append(row[\"count(w1,w2)\"]/unigram_data.iloc[row[\"w1\"]-1][\"Count\"])\n",
    "bigram_data[\"Bigram Probs\"] = bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"THE STOCK MARKET FELL BY ONE HUNDRED POINTS LAST WEEK\"\n",
    "sentence1 = sentence1.split()\n",
    "indeces1 = []\n",
    "for word in sentence1:\n",
    "    index = vocab_data.index\n",
    "    condition = vocab_data[\"Words\"] == word\n",
    "    word_index = index[condition]\n",
    "    indeces1.append(word_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram log-likelihood: -64.50944\n"
     ]
    }
   ],
   "source": [
    "# Unigram log-likelihood\n",
    "Lu = 0\n",
    "for index in indeces1:\n",
    "    Lu += np.log(vocab_data.iloc[index][\"Unigram Probs\"])\n",
    "print(f\"Unigram log-likelihood: {np.round(Lu,5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"<s> THE STOCK MARKET FELL BY ONE HUNDRED POINTS LAST WEEK\"\n",
    "sentence2 = sentence2.split()\n",
    "indeces2 = []\n",
    "for word in sentence2:\n",
    "    index = vocab_data.index\n",
    "    condition = vocab_data[\"Words\"] == word\n",
    "    word_index = index[condition]+1\n",
    "    indeces2.append(word_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram log-likelihood: -40.91813\n"
     ]
    }
   ],
   "source": [
    "# Bigram log-likelihood\n",
    "Lb = 0\n",
    "for i in range(len(indeces2)-1):\n",
    "    # Last word on the line is .values[0]\n",
    "    Lb+=np.log(bigram_data[(bigram_data[\"w1\"]==indeces2[i])&(bigram_data[\"w2\"]==indeces2[i+1])][\"Bigram Probs\"].values[0])\n",
    "print(f\"Bigram log-likelihood: {np.round(Lb,5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the bigram model yields a better log-likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)**\n",
    "\n",
    "Consider the sentence **“The sixteen officials sold fire insurance.”** Ignoring punctuation,\n",
    "compute and compare the log-likelihoods of this sentence under the unigram and bigram models:\n",
    "\n",
    "$$\\mathcal{L}_{u} = log[P_{u}\\bf(The)P_{u}\\bf(sixteen)P_{u}\\bf(officials)...P_{u}\\bf(fire)P_{u}\\bf(insurance)]$$\n",
    "$$\\mathcal{L}_{b} = log[P_{b}\\bf(The|<s>)P_{b}\\bf(sixteen|the)P_{b}\\bf(officials|sixteen)...P_{b}\\bf(fire|sold)P_{b}\\bf(insurance|fire)]$$\n",
    "\n",
    "Which pairs of adjacent words in this sentence are not observed in the training corpus? What effect\n",
    "does this have on the log-likelihood from the bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence3 = \"THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE\"\n",
    "sentence3 = sentence3.split()\n",
    "indeces3 = []\n",
    "for word in sentence3:\n",
    "    index = vocab_data.index\n",
    "    condition = vocab_data[\"Words\"] == word\n",
    "    word_index = index[condition]\n",
    "    indeces3.append(word_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram log-likelihood: -44.29193\n"
     ]
    }
   ],
   "source": [
    "# Unigram log-likelihood\n",
    "Lu = 0\n",
    "for index in indeces3:\n",
    "    Lu += np.log(vocab_data.iloc[index][\"Unigram Probs\"])\n",
    "print(f\"Unigram log-likelihood: {np.round(Lu,5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence4 = \"<s> THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE\"\n",
    "sentence4 = sentence4.split()\n",
    "indeces4 = []\n",
    "for word in sentence4:\n",
    "    index = vocab_data.index\n",
    "    condition = vocab_data[\"Words\"] == word\n",
    "    word_index = index[condition]+1\n",
    "    indeces4.append(word_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination of SIXTEEN and OFFICIALS is not present in the bigram data, resulting in log(0) which is -inf\n",
      "The combination of SOLD and FIRE is not present in the bigram data, resulting in log(0) which is -inf\n",
      "Bigram log-likelihood: -inf\n"
     ]
    }
   ],
   "source": [
    "# Bigram log-likelihood\n",
    "Lb = 0\n",
    "for i in range(len(indeces4)-1):\n",
    "    try:\n",
    "        # Last word on the line is .values[0]\n",
    "        prob = bigram_data[(bigram_data[\"w1\"]==indeces4[i])&(bigram_data[\"w2\"]==indeces4[i+1])][\"Bigram Probs\"].values[0]\n",
    "    except IndexError:\n",
    "        print(f\"The combination of {sentence4[i]} and {sentence4[i+1]} is not present in the bigram data, resulting in log(0) which is -inf\")\n",
    "        Lb = float('-inf')\n",
    "    else:\n",
    "        Lb+=np.log(prob)\n",
    "print(f\"Bigram log-likelihood: {np.round(Lb,5)}\")\n",
    "#print(f\"The combination of {sentence4[i]} and {sentence4[i+1]} is not present in the bigram data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)**\n",
    "\n",
    "Consider the so-called mixture model that predicts words from a weighted interpolation of the unigram\n",
    "and bigram models: $$P_{m}(w'|w) = \\lambda P_{u}(w') + (1-\\lambda)P_{b}(w'|w)$$ where $\\lambda \\in [0,1]$ determines how much weight is attached to each prediction. Under this mixture model, the log-likelihood of the sentence from part (d) is given by: $$\\mathcal{L}_{m} = log[P_{m}\\bf(The|<s>)P_{m}\\bf(sixteen|the)P_{m}\\bf(officials|sixteen) ... P_{m}\\bf(fire|sold)P_{m}\\bf(insurance|fire)].$$ Compute and plot the value of this log-likelihood $\\mathcal{L}_{m}$ as a function of the parameter $\\lambda \\in [0,1]$. From your results, deduce the optimal value of $\\lambda$ to two significant digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest lambda value is 0.65\n"
     ]
    }
   ],
   "source": [
    "sentence5 = \"<s> THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE\"\n",
    "sentence5 = sentence5.split()\n",
    "indeces5 = []\n",
    "for word in sentence5:\n",
    "    index = vocab_data.index\n",
    "    condition = vocab_data[\"Words\"] == word\n",
    "    word_index = index[condition]+1\n",
    "    indeces5.append(word_index[0])\n",
    "\n",
    "def mixture_prob(lam, p_uni, p_bi):\n",
    "    return (np.log(lam*p_uni+(1-lam)*p_bi))\n",
    "\n",
    "def mixture_mle(sentence5, lam):\n",
    "    p = 0\n",
    "    for i in range(len(sentence5) - 1):\n",
    "        p_uni = vocab_data[vocab_data[\"Words\"]==sentence5[i+1]][\"Unigram Probs\"].values[0]\n",
    "        try:\n",
    "            # Last word on the line is .values[0]\n",
    "            p_bi=bigram_data[(bigram_data[\"w1\"]==indeces5[i])&(bigram_data[\"w2\"]==indeces5[i+1])][\"Bigram Probs\"].values[0]\n",
    "        except IndexError:\n",
    "            p_bi = 0\n",
    "        p_ = mixture_prob(lam,p_uni,p_bi)\n",
    "        p += p_\n",
    "    return p\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in np.linspace(0.01, 1, 100):\n",
    "    x.append(i)\n",
    "    y.append(mixture_mle(sentence5, i))\n",
    "plt.plot(x, y, 'b-.')\n",
    "plt.show()\n",
    "\n",
    "np.array(x)\n",
    "np.array(y)\n",
    "print(f\"Highest lambda value is {round(x[np.argmax(y)],2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
